# Models

Here, we describe the pre-trained models we provide based on our PigeonSuperModel dataset. We trained multiple [DeepLabCut](https://deeplabcut.github.io/DeepLabCut) models based on different architectures (`resnet-50`, `resneet-101`, `resnet-152`) and compared tracking performance across different training stages. We make the final, pre-trained models available for out-of-the-box analysis of new videos. Moreover, we used the same dataset to train UNet models in [SLEAP](https://sleap.ai/) and benchmark performance differences, training and inference rates.

## DeeplabCut

Using DeepLabCut, we trained three different network architectures to compare training and inference times, as well as overall tracking performance on known and unseen frames.

### ResNet-50

Coming soon! We are preparing a short pre-print describing model training and configuration parameters. In the meantime, use [this link](https://gitlab.ruhr-uni-bochum.de/ikn/pigeonsupermodel/-/tree/main/models/DeepLabCut) to download the pre-trained model.

### ResNet-101

Coming soon! We are preparing a short pre-print describing model training and configuration parameters. In the meantime, use [this link](https://gitlab.ruhr-uni-bochum.de/ikn/pigeonsupermodel/-/tree/main/models/DeepLabCut) to download the pre-trained model.

### ResNet-152

Coming soon! We are preparing a short pre-print describing model training and configuration parameters. In the meantime, use [this link](https://gitlab.ruhr-uni-bochum.de/ikn/pigeonsupermodel/-/tree/main/models/DeepLabCut) to download the pre-trained model.

## SLEAP

:::{note}
This section is coming soon, we would like to polish the pre-print first.
:::

### ResNet

:::{note}
This section is coming soon, we would like to polish the pre-print first.
:::

### UNet

:::{note}
This section is coming soon, we would like to polish the pre-print first.
:::
